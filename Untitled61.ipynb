{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hatimdeve/python/blob/main/Untitled61.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpnpRTuVvsE7",
        "outputId": "688fc0bf-9d20-4b2c-927c-f0ac7d961608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-2ef45344-9c7c-1cda-8013-c26befc3f42b)\n"
          ]
        }
      ],
      "source": [
        "# Get GPU name\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaQJiXvza0KX",
        "outputId": "a2b9f0f4-4a01-4944-f01d-5367b3780e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.13.* (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.13.0rc0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.13.*\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.13.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2QTyopxaw5Y",
        "outputId": "46c4d81a-962e-4b8b-e290-87cc5e9ae0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.14.0-dev20230524\n",
            "Notebook last run (end-to-end): 2023-05-24 21:35:46.686313\n"
          ]
        }
      ],
      "source": [
        "# Note: As of May 2023, there have been some issues with TensorFlow versions 2.9-2.12\n",
        "# with the following code. \n",
        "# However, these seemed to have been fixed in version 2.13+.\n",
        "# TensorFlow version 2.13 is available in tf-nightly as of May 2023 (will be default in Google Colab soon).\n",
        "# Therefore, to prevent errors we'll install tf-nightly first.\n",
        "# See more here: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/550 \n",
        "\n",
        "# Install tf-nightly (required until 2.13.0+ is the default in Google Colab)\n",
        "!pip install -U -q tf-nightly\n",
        "\n",
        "# Check TensorFlow version (should be minimum 2.4.0+ but 2.13.0+ is better)\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Add timestamp\n",
        "import datetime\n",
        "print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hyHMLx0YzeTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f3e759-47eb-4a78-ea68-ffdd4f803187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 'helper_functions.py' already exists, skipping download.\n"
          ]
        }
      ],
      "source": [
        "# Get helper functions file\n",
        "import os \n",
        "\n",
        "if not os.path.exists(\"helper_functions.py\"):\n",
        "    !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "else:\n",
        "    print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tDWijXApzqSq"
      },
      "outputs": [],
      "source": [
        "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6SiMotw-zzVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafa1cf9-2515-46bb-96f1-eb8ba087901d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/Drive; to attempt to forcibly remount, call drive.mount(\"/content/Drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VIfN4pbT98gM"
      },
      "outputs": [],
      "source": [
        "path = \"/content/Drive/MyDrive/my_dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "to_SxxeV-B0g"
      },
      "outputs": [],
      "source": [
        "# Create training and test directories\n",
        "train_dir = \"/content/Drive/MyDrive/my_dataset/CNN_project/train/\"\n",
        "test_dir = \"/content/Drive/MyDrive/my_dataset/CNN_project/test/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hk8wdV-qPr-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8543ba68-8f30-4150-9263-26eba5323e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12447 files belonging to 67 classes.\n",
            "Found 3144 files belonging to 67 classes.\n"
          ]
        }
      ],
      "source": [
        "# Create data inputs\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224, 224) # define image size\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
        "                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                                           image_size=IMG_SIZE,\n",
        "                                                                           label_mode=\"categorical\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_Ft2KyYmwWdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a9bbc8-2c09-4e53-d30d-7ba99326949a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 67), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3ktJr2MFoKJX"
      },
      "outputs": [],
      "source": [
        "# Import the required modules for model creation\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "## NEW: Newer versions of TensorFlow (2.10+) can use the tensorflow.keras.layers API directly for data augmentation\n",
        "data_augmentation = tf.keras.models.Sequential([\n",
        "  layers.RandomFlip(\"horizontal\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  layers.RandomZoom(0.2),\n",
        "  layers.RandomHeight(0.2),\n",
        "  layers.RandomWidth(0.2),\n",
        "  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetB0\n",
        "], name =\"data_augmentation\")\n",
        "\n",
        "## OLD\n",
        "# # Setup data augmentation\n",
        "# from tensorflow.keras.layers.experimental import preprocessing\n",
        "# data_augmentation = Sequential([\n",
        "#   preprocessing.RandomFlip(\"horizontal\"), # randomly flip images on horizontal edge\n",
        "#   preprocessing.RandomRotation(0.2), # randomly rotate images by a specific amount\n",
        "#   preprocessing.RandomHeight(0.2), # randomly adjust the height of an image by a specific amount\n",
        "#   preprocessing.RandomWidth(0.2), # randomly adjust the width of an image by a specific amount\n",
        "#   preprocessing.RandomZoom(0.2), # randomly zoom into an image\n",
        "#   # preprocessing.Rescaling(1./255) # keep for models like ResNet50V2, remove for EfficientNet\n",
        "# ], name=\"data_augmentation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r9CyBezKoOu7"
      },
      "outputs": [],
      "source": [
        "# Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create ModelCheckpoint callback to save model's progress\n",
        "checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
        "                                                      save_best_only=True, # only save the best weights\n",
        "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                      verbose=0) # don't print out whether or not model is being saved "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_xgsrnAH_Xtq"
      },
      "outputs": [],
      "source": [
        "# Turn on mixed precision training\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gtVybyWgJAtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4042342f-2c8b-484d-a680-4479971423c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Policy \"mixed_float16\">"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "mixed_precision.global_policy() # should output \"mixed_float16\" (if your GPU is compatible with mixed precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hzfmqPOuJExj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create base model\n",
        "input_shape = (224, 224, 3)\n",
        "# Setup base model and freeze its layers (this will extract features)\n",
        "base_model = tf.keras.applications.resnet50.ResNet50(include_top=False)\n",
        "base_model.trainable = False # freeze base model layers\n",
        "\n",
        "# Create Functional model \n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
        "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
        "x = layers.Dense(len(train_data.class_names))(x) # want one output neuron per class \n",
        "# Separate activation of output layer so we can output float32 activations\n",
        "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T4OlqwGlFZSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e39f970-5343-4df4-9b7a-a79fef500686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, None, None, 2048   23587712  \n",
            "                             )                                   \n",
            "                                                                 \n",
            " pooling_layer (GlobalAvera  (None, 2048)              0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 67)                137283    \n",
            "                                                                 \n",
            " softmax_float32 (Activatio  (None, 67)                0         \n",
            " n)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23724995 (90.50 MB)\n",
            "Trainable params: 137283 (536.26 KB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Check out our model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "04ZteWzX0Mum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc49d2c-144c-4349-8ee0-178868cfc7cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_layer True float32 <Policy \"float32\">\n",
            "resnet50 False float32 <Policy \"mixed_float16\">\n",
            "pooling_layer True float32 <Policy \"mixed_float16\">\n",
            "dense True float32 <Policy \"mixed_float16\">\n",
            "softmax_float32 True float32 <Policy \"float32\">\n"
          ]
        }
      ],
      "source": [
        "# Check the dtype_policy attributes of layers in our model\n",
        "for layer in model.layers:\n",
        "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which layers are trainable\n",
        "for layer_number, layer in enumerate(base_model.layers):\n",
        "  print(layer_number, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "SLGi7wGj_Wqv",
        "outputId": "e0ed4ca6-ec2e-4bc0-e36a-978e61f4148a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_1 False\n",
            "1 conv1_pad False\n",
            "2 conv1_conv False\n",
            "3 conv1_bn False\n",
            "4 conv1_relu False\n",
            "5 pool1_pad False\n",
            "6 pool1_pool False\n",
            "7 conv2_block1_1_conv False\n",
            "8 conv2_block1_1_bn False\n",
            "9 conv2_block1_1_relu False\n",
            "10 conv2_block1_2_conv False\n",
            "11 conv2_block1_2_bn False\n",
            "12 conv2_block1_2_relu False\n",
            "13 conv2_block1_0_conv False\n",
            "14 conv2_block1_3_conv False\n",
            "15 conv2_block1_0_bn False\n",
            "16 conv2_block1_3_bn False\n",
            "17 conv2_block1_add False\n",
            "18 conv2_block1_out False\n",
            "19 conv2_block2_1_conv False\n",
            "20 conv2_block2_1_bn False\n",
            "21 conv2_block2_1_relu False\n",
            "22 conv2_block2_2_conv False\n",
            "23 conv2_block2_2_bn False\n",
            "24 conv2_block2_2_relu False\n",
            "25 conv2_block2_3_conv False\n",
            "26 conv2_block2_3_bn False\n",
            "27 conv2_block2_add False\n",
            "28 conv2_block2_out False\n",
            "29 conv2_block3_1_conv False\n",
            "30 conv2_block3_1_bn False\n",
            "31 conv2_block3_1_relu False\n",
            "32 conv2_block3_2_conv False\n",
            "33 conv2_block3_2_bn False\n",
            "34 conv2_block3_2_relu False\n",
            "35 conv2_block3_3_conv False\n",
            "36 conv2_block3_3_bn False\n",
            "37 conv2_block3_add False\n",
            "38 conv2_block3_out False\n",
            "39 conv3_block1_1_conv False\n",
            "40 conv3_block1_1_bn False\n",
            "41 conv3_block1_1_relu False\n",
            "42 conv3_block1_2_conv False\n",
            "43 conv3_block1_2_bn False\n",
            "44 conv3_block1_2_relu False\n",
            "45 conv3_block1_0_conv False\n",
            "46 conv3_block1_3_conv False\n",
            "47 conv3_block1_0_bn False\n",
            "48 conv3_block1_3_bn False\n",
            "49 conv3_block1_add False\n",
            "50 conv3_block1_out False\n",
            "51 conv3_block2_1_conv False\n",
            "52 conv3_block2_1_bn False\n",
            "53 conv3_block2_1_relu False\n",
            "54 conv3_block2_2_conv False\n",
            "55 conv3_block2_2_bn False\n",
            "56 conv3_block2_2_relu False\n",
            "57 conv3_block2_3_conv False\n",
            "58 conv3_block2_3_bn False\n",
            "59 conv3_block2_add False\n",
            "60 conv3_block2_out False\n",
            "61 conv3_block3_1_conv False\n",
            "62 conv3_block3_1_bn False\n",
            "63 conv3_block3_1_relu False\n",
            "64 conv3_block3_2_conv False\n",
            "65 conv3_block3_2_bn False\n",
            "66 conv3_block3_2_relu False\n",
            "67 conv3_block3_3_conv False\n",
            "68 conv3_block3_3_bn False\n",
            "69 conv3_block3_add False\n",
            "70 conv3_block3_out False\n",
            "71 conv3_block4_1_conv False\n",
            "72 conv3_block4_1_bn False\n",
            "73 conv3_block4_1_relu False\n",
            "74 conv3_block4_2_conv False\n",
            "75 conv3_block4_2_bn False\n",
            "76 conv3_block4_2_relu False\n",
            "77 conv3_block4_3_conv False\n",
            "78 conv3_block4_3_bn False\n",
            "79 conv3_block4_add False\n",
            "80 conv3_block4_out False\n",
            "81 conv4_block1_1_conv False\n",
            "82 conv4_block1_1_bn False\n",
            "83 conv4_block1_1_relu False\n",
            "84 conv4_block1_2_conv False\n",
            "85 conv4_block1_2_bn False\n",
            "86 conv4_block1_2_relu False\n",
            "87 conv4_block1_0_conv False\n",
            "88 conv4_block1_3_conv False\n",
            "89 conv4_block1_0_bn False\n",
            "90 conv4_block1_3_bn False\n",
            "91 conv4_block1_add False\n",
            "92 conv4_block1_out False\n",
            "93 conv4_block2_1_conv False\n",
            "94 conv4_block2_1_bn False\n",
            "95 conv4_block2_1_relu False\n",
            "96 conv4_block2_2_conv False\n",
            "97 conv4_block2_2_bn False\n",
            "98 conv4_block2_2_relu False\n",
            "99 conv4_block2_3_conv False\n",
            "100 conv4_block2_3_bn False\n",
            "101 conv4_block2_add False\n",
            "102 conv4_block2_out False\n",
            "103 conv4_block3_1_conv False\n",
            "104 conv4_block3_1_bn False\n",
            "105 conv4_block3_1_relu False\n",
            "106 conv4_block3_2_conv False\n",
            "107 conv4_block3_2_bn False\n",
            "108 conv4_block3_2_relu False\n",
            "109 conv4_block3_3_conv False\n",
            "110 conv4_block3_3_bn False\n",
            "111 conv4_block3_add False\n",
            "112 conv4_block3_out False\n",
            "113 conv4_block4_1_conv False\n",
            "114 conv4_block4_1_bn False\n",
            "115 conv4_block4_1_relu False\n",
            "116 conv4_block4_2_conv False\n",
            "117 conv4_block4_2_bn False\n",
            "118 conv4_block4_2_relu False\n",
            "119 conv4_block4_3_conv False\n",
            "120 conv4_block4_3_bn False\n",
            "121 conv4_block4_add False\n",
            "122 conv4_block4_out False\n",
            "123 conv4_block5_1_conv False\n",
            "124 conv4_block5_1_bn False\n",
            "125 conv4_block5_1_relu False\n",
            "126 conv4_block5_2_conv False\n",
            "127 conv4_block5_2_bn False\n",
            "128 conv4_block5_2_relu False\n",
            "129 conv4_block5_3_conv False\n",
            "130 conv4_block5_3_bn False\n",
            "131 conv4_block5_add False\n",
            "132 conv4_block5_out False\n",
            "133 conv4_block6_1_conv False\n",
            "134 conv4_block6_1_bn False\n",
            "135 conv4_block6_1_relu False\n",
            "136 conv4_block6_2_conv False\n",
            "137 conv4_block6_2_bn False\n",
            "138 conv4_block6_2_relu False\n",
            "139 conv4_block6_3_conv False\n",
            "140 conv4_block6_3_bn False\n",
            "141 conv4_block6_add False\n",
            "142 conv4_block6_out False\n",
            "143 conv5_block1_1_conv False\n",
            "144 conv5_block1_1_bn False\n",
            "145 conv5_block1_1_relu False\n",
            "146 conv5_block1_2_conv False\n",
            "147 conv5_block1_2_bn False\n",
            "148 conv5_block1_2_relu False\n",
            "149 conv5_block1_0_conv False\n",
            "150 conv5_block1_3_conv False\n",
            "151 conv5_block1_0_bn False\n",
            "152 conv5_block1_3_bn False\n",
            "153 conv5_block1_add False\n",
            "154 conv5_block1_out False\n",
            "155 conv5_block2_1_conv False\n",
            "156 conv5_block2_1_bn False\n",
            "157 conv5_block2_1_relu False\n",
            "158 conv5_block2_2_conv False\n",
            "159 conv5_block2_2_bn False\n",
            "160 conv5_block2_2_relu False\n",
            "161 conv5_block2_3_conv False\n",
            "162 conv5_block2_3_bn False\n",
            "163 conv5_block2_add False\n",
            "164 conv5_block2_out False\n",
            "165 conv5_block3_1_conv False\n",
            "166 conv5_block3_1_bn False\n",
            "167 conv5_block3_1_relu False\n",
            "168 conv5_block3_2_conv False\n",
            "169 conv5_block3_2_bn False\n",
            "170 conv5_block3_2_relu False\n",
            "171 conv5_block3_3_conv False\n",
            "172 conv5_block3_3_bn False\n",
            "173 conv5_block3_add False\n",
            "174 conv5_block3_out False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CzddD5OU0MwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a147507-d720-4d75-ca21-42bc53990d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_1 False float32 <Policy \"float32\">\n",
            "conv1_pad False float32 <Policy \"mixed_float16\">\n",
            "conv1_conv False float32 <Policy \"mixed_float16\">\n",
            "conv1_bn False float32 <Policy \"mixed_float16\">\n",
            "conv1_relu False float32 <Policy \"mixed_float16\">\n",
            "pool1_pad False float32 <Policy \"mixed_float16\">\n",
            "pool1_pool False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_1_conv False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_1_bn False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_1_relu False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_2_conv False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_2_bn False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_2_relu False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_0_conv False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_3_conv False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_0_bn False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_3_bn False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_add False float32 <Policy \"mixed_float16\">\n",
            "conv2_block1_out False float32 <Policy \"mixed_float16\">\n",
            "conv2_block2_1_conv False float32 <Policy \"mixed_float16\">\n"
          ]
        }
      ],
      "source": [
        "# Check the layers in the base model and see what dtype policy they're using\n",
        "for layer in model.layers[1].layers[:20]: # only check the first 20 layers to save output space\n",
        "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EoXqNv7f0VHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b768f40-0b4e-4151-d527-cc7766f40242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientnetb0_101_classes_all_data_feature_extract/20230524-213610\n",
            "Epoch 1/3\n",
            "389/389 [==============================] - 1605s 4s/step - loss: 1.7985 - accuracy: 0.5237 - val_loss: 1.3435 - val_accuracy: 0.6205\n",
            "Epoch 2/3\n",
            "389/389 [==============================] - 58s 148ms/step - loss: 0.9197 - accuracy: 0.7328 - val_loss: 1.2700 - val_accuracy: 0.6451\n",
            "Epoch 3/3\n",
            "389/389 [==============================] - 55s 138ms/step - loss: 0.6651 - accuracy: 0.8050 - val_loss: 1.3607 - val_accuracy: 0.6272\n"
          ]
        }
      ],
      "source": [
        "# Turn off all warnings except for errors\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Fit the model with callbacks\n",
        "history_classes_feature_extract = model.fit(train_data, \n",
        "                                                     epochs=3,\n",
        "                                                     steps_per_epoch=len(train_data),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(\"training_logs\", \n",
        "                                                                                            \"all_data_feature_extract\"),\n",
        "                                                                model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mBl9p9y_83fZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9beb37c3-0cb1-44f1-ac9c-b9c9f5fb7731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99/99 [==============================] - 470s 5s/step - loss: 1.2305 - accuracy: 0.6606\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2305227518081665, 0.6606234312057495]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Evaluate model (unsaved version) on whole test dataset\n",
        "results_feature_extract_model = model.evaluate(test_data)\n",
        "results_feature_extract_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHirew2Z-Hm9"
      },
      "outputs": [],
      "source": [
        "# 1. Create a function to recreate the original model\n",
        "def create_model():\n",
        "  # Create base model\n",
        "  input_shape = (224, 224, 3)\n",
        "  base_model = tf.keras.applications.resnet50.ResNet50(include_top=False)\n",
        "  base_model.trainable = False # freeze base model layers\n",
        "\n",
        "  # Create Functional model \n",
        "  inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "  # Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
        "  x = layers.Rescaling(1./255)(x)\n",
        "  x = base_model(inputs, training=False) # set base_model to inference mode only\n",
        "  x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
        "  x = layers.Dense(len(train_data.class_names))(x) # want one output neuron per class \n",
        "  # Separate activation of output layer so we can output float32 activations\n",
        "  outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  \n",
        "  return model\n",
        "\n",
        "# 2. Create and compile a new version of the original model (new weights)\n",
        "created_model = create_model()\n",
        "created_model.compile(loss=\"categorical_crossentropy\",\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "# 3. Load the saved weights\n",
        "created_model.load_weights(checkpoint_path)\n",
        "\n",
        "# 4. Evaluate the model with loaded weights\n",
        "results_created_model_with_loaded_weights = created_model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ufvt7sOqAfpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c030bcdc-0bfc-42e1-d2fc-0c95bb3a78ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_layer True float32 <Policy \"float32\">\n",
            "resnet50 True float32 <Policy \"mixed_float16\">\n",
            "pooling_layer True float32 <Policy \"mixed_float16\">\n",
            "dense_1 True float32 <Policy \"mixed_float16\">\n",
            "softmax_float32 True float32 <Policy \"float32\">\n"
          ]
        }
      ],
      "source": [
        "# Are any of the layers in our model frozen?\n",
        "for layer in created_model.layers:\n",
        "    layer.trainable = True # set all layers to trainable\n",
        "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # make sure loaded model is using mixed precision dtype_policy (\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX-Sid4lAqPz"
      },
      "outputs": [],
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "checkpoint_path = \"fine_tune_checkpoints/cp.ckpt\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor=\"val_loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tG4kSj9EAywq"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss=\"categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n",
        "                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n",
        "                        metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oIsheAU5BBiG"
      },
      "outputs": [],
      "source": [
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down \n",
        "                                                 min_lr=1e-7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zwXodCyHA4Yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684178a7-7878-418f-be8f-a43b5a974c6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientb0_101_classes_all_data_fine_tuning/20230524-221354\n",
            "Epoch 1/100\n",
            "389/389 [==============================] - 62s 145ms/step - loss: 0.4058 - accuracy: 0.8956 - val_loss: 1.0730 - val_accuracy: 0.6964 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "389/389 [==============================] - 53s 135ms/step - loss: 0.3794 - accuracy: 0.9045 - val_loss: 1.0464 - val_accuracy: 0.7009 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "389/389 [==============================] - 54s 137ms/step - loss: 0.3605 - accuracy: 0.9155 - val_loss: 1.0872 - val_accuracy: 0.6942 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "389/389 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.9215\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "389/389 [==============================] - 53s 135ms/step - loss: 0.3442 - accuracy: 0.9215 - val_loss: 1.0529 - val_accuracy: 0.6964 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "389/389 [==============================] - 55s 140ms/step - loss: 0.3202 - accuracy: 0.9319 - val_loss: 1.1773 - val_accuracy: 0.6607 - lr: 2.0000e-05\n"
          ]
        }
      ],
      "source": [
        "# Start to fine-tune (all layers)\n",
        "history_fine_tune = model.fit(train_data,\n",
        "                                                        epochs=100, # fine-tune for a maximum of 100 epochs\n",
        "                                                        steps_per_epoch=len(train_data),\n",
        "                                                        validation_data=test_data,\n",
        "                                                        validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n",
        "                                                        callbacks=[create_tensorboard_callback(\"training_logs\", \"all_data_fine_tuning\"), # track the model training logs\n",
        "                                                                   early_stopping, # stop model after X epochs of no improvements\n",
        "                                                                   reduce_lr]) # reduce the learning rate after X epochs of no improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OIpJv9xrj98S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60295350-9735-40f2-c957-2b1dd7e3f9c9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99/99 [==============================] - 12s 110ms/step - loss: 1.0713 - accuracy: 0.7010\n"
          ]
        }
      ],
      "source": [
        "# 4. Evaluate the model with loaded weights\n",
        "results_created_model_with_loaded_weights = model.evaluate(test_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxxb4dHVVy3uZ92S+4s0ul",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}